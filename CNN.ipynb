{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Detecting facial keypoints tutorial using convolutional neural nets</Font>\n",
    "\n",
    "We'll use Lasagne(a new library for building neural networks with Python and Theano) to implement a couple of network architectures, talk about data augmentation, dropout, the importance of momentum, and pre-training.\n",
    "\n",
    "### <font color='blue'>The data</font>\n",
    "\n",
    "The training dataset for the Facial Keypoint Detection challenge consists of 7,049 96x96 gray-scale images. For each image, we're supposed learn to find the correct position (the x and y coordinates) of 15 keypoints, such as left_eye_center, right_eye_outer_corner, mouth_center_bottom_lip, and so on.\n",
    "\n",
    "![im](img/1.png)\n",
    "\n",
    "A point to note is,for some of the keypoints we only have about 2,000 labels, while other keypoints have more than 7,000 labels available for training.\n",
    "\n",
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_eye_center_x            7039\n",
      "left_eye_center_y            7039\n",
      "right_eye_center_x           7036\n",
      "right_eye_center_y           7036\n",
      "left_eye_inner_corner_x      2271\n",
      "left_eye_inner_corner_y      2271\n",
      "left_eye_outer_corner_x      2267\n",
      "left_eye_outer_corner_y      2267\n",
      "right_eye_inner_corner_x     2268\n",
      "right_eye_inner_corner_y     2268\n",
      "right_eye_outer_corner_x     2268\n",
      "right_eye_outer_corner_y     2268\n",
      "left_eyebrow_inner_end_x     2270\n",
      "left_eyebrow_inner_end_y     2270\n",
      "left_eyebrow_outer_end_x     2225\n",
      "left_eyebrow_outer_end_y     2225\n",
      "right_eyebrow_inner_end_x    2270\n",
      "right_eyebrow_inner_end_y    2270\n",
      "right_eyebrow_outer_end_x    2236\n",
      "right_eyebrow_outer_end_y    2236\n",
      "nose_tip_x                   7049\n",
      "nose_tip_y                   7049\n",
      "mouth_left_corner_x          2269\n",
      "mouth_left_corner_y          2269\n",
      "mouth_right_corner_x         2270\n",
      "mouth_right_corner_y         2270\n",
      "mouth_center_top_lip_x       2275\n",
      "mouth_center_top_lip_y       2275\n",
      "mouth_center_bottom_lip_x    7016\n",
      "mouth_center_bottom_lip_y    7016\n",
      "Image                        7049\n",
      "dtype: int64\n",
      "X.shape == (2140, 9216); X.min == 0.000; X.max == 1.000\n",
      "y.shape == (2140, 30); y.min == -0.920; y.max == 0.996\n"
     ]
    }
   ],
   "source": [
    "# file kfkd.py\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "FTRAIN = 'training.csv'\n",
    "FTEST = 'test.csv'\n",
    "\n",
    "\n",
    "def load(test=False, cols=None):\n",
    "    \"\"\"Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n",
    "    Pass a list of *cols* if you're only interested in a subset of the\n",
    "    target columns.\n",
    "    \"\"\"\n",
    "    fname = FTEST if test else FTRAIN\n",
    "    df = read_csv(os.path.expanduser(fname))  # load pandas dataframe\n",
    "\n",
    "    # The Image column has pixel values separated by space; convert\n",
    "    # the values to numpy arrays:\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))\n",
    "\n",
    "    if cols:  # get a subset of columns\n",
    "        df = df[list(cols) + ['Image']]\n",
    "\n",
    "    print(df.count())  # prints the number of values for each column\n",
    "    df = df.dropna()  # drop all rows that have missing values in them\n",
    "\n",
    "    X = np.vstack(df['Image'].values) / 255.  # scale pixel values to [0, 1]\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    if not test:  # only FTRAIN has any target columns\n",
    "        y = df[df.columns[:-1]].values\n",
    "        y = (y - 48) / 48  # scale target coordinates to [-1, 1]\n",
    "        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n",
    "        y = y.astype(np.float32)\n",
    "    else:\n",
    "        y = None\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = load()\n",
    "print(\"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\".format(\n",
    "    X.shape, X.min(), X.max()))\n",
    "print(\"y.shape == {}; y.min == {:.3f}; y.max == {:.3f}\".format(\n",
    "    y.shape, y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while we have an Image for all rows in the training data, we only have 2,267 values for mouth_right_corner_x and so on.\n",
    "\n",
    "load() returns a tuple (X, y) where y is the target matrix. y has shape n x m with n being the number of samples in the dataset that have all m keypoints. Dropping all rows with missing values is what this line does:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # drop all rows that have missing values in them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output y.shape == (2140, 30) tells us that there are 2,140 images in the dataset that have all 30 target values present. Initially, we'll train these 2,140 samples only.\n",
    "\n",
    "This leaves us with many more input dimensions (9,216) than samples; an indicator that overfitting might become a problem.\n",
    "\n",
    "Another feature of the load() function is that it scales the intensity values of the image pixels to be in the interval [0,1], instead of 0 to 255. The target values (x and y coordinates) are scaled to [-1, 1]; before they were between 0 to 95.\n",
    "\n",
    "### <font color='blue'>First model: a single hidden layer</font>\n",
    "Now let's use Lasagne and create a neural net with a single hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to kfkd.py\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "    hidden_num_units=100,  # number of units in hidden layer\n",
    "    output_nonlinearity=None,  # output layer uses identity function\n",
    "    output_num_units=30,  # 30 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=True,  # flag to indicate we're dealing with regression problem\n",
    "    max_epochs=400,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "X, y = load()\n",
    "net1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a few parameters to initialize the NeuralNet. Let's walk through them. So we have the three layers and their parameters:\n",
    "\n",
    "<code> layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "<code>   # layer parameters:\n",
    "    input_shape=(None, 9216),  # 96x96 input pixels per batch\n",
    "    hidden_num_units=100,  # number of units in hidden layer\n",
    "    output_nonlinearity=None,  # output layer uses identity function\n",
    "    output_num_units=30,  # 30 target values\n",
    "</code>\n",
    "    \n",
    "Here we define the input layer, the hidden layer and the output layer. In parameter layers, we name and specify the type of each layer, and their order. Parameters input_shape, hidden_num_units, output_nonlinearity, and output_num_units are each parameters for specific layers; they refer to the layer by their prefix, such that input_shape defines the shape parameter of the input layer, hidden_num_units defines the hidden layer's num_units and so on.\n",
    "\n",
    "We set the first dimension of input_shape to None. This translates to a variable batch size.\n",
    "\n",
    "We set the output_nonlinearity to None explicitly. Thus, the output units' activations become just a linear combination of the activations in the hidden layer.\n",
    "\n",
    "The default nonlinearity used by DenseLayer is the rectifier, which is simply max(0, x). By not explicitly setting hidden_nonlinearity, we're choosing the rectifier as the activiation function of our hidden layer.\n",
    "![imn](img/2.png)\n",
    "The neural net's weights are initialized from a uniform distribution with a cleverly chosen interval. That is, Lasagne figures out this interval for us, using \"Glorot-style\" initialization.\n",
    "\n",
    "All parameters starting with update parametrize the update function, or optimization method. The update function will update the weights of our network after each batch. We'll use the nesterov_momentum gradient descent optimization method to do the job. There's a number of other methods that Lasagne implements, such as adagrad and rmsprop. We choose nesterov_momentum because it has proven to work very well for a large number of problems.\n",
    "\n",
    "<code>    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,</code>\n",
    "   \n",
    "The update_learning_rate defines how large we want the steps of the gradient descent updates to be. We'll talk a bit more about the learning_rate and momentum parameters later on. For now, it's enough to just use these \"sane defaults.\"\n",
    "\n",
    "![gif](img/3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><span style=\"color:red; font-family:Georgia; text-align:center; \">\n",
    "    Comparison of a few optimization methods (animation by Alec Radford).<br>\n",
    "    The star denotes the global minimum on the error surface. <br>\n",
    "    Notice that stochastic gradient descent (SGD) without momentum is the slowest method to converge in this example. <br>\n",
    "    We're using Nesterov's Accelerated Gradient Descent (NAG) throughout this tutorial.<br>\n",
    "    </span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for regression problems mean squared error (MSE) is used to specify an objective function to minimize NeuralNet.\n",
    "The last set of parameters declare that we're dealing with a regression problem (as opposed to classification), that 400 is the number of epochs we're willing to train, and that we want to print out information during training by setting verbose=1:\n",
    "\n",
    "```\n",
    "regression=True,  # flag to indicate we're dealing with regression problem\n",
    "  max_epochs=400,  # we want to train this many epochs\n",
    "  verbose=1,\n",
    "```\n",
    "  \n",
    "The last two lines in our script load the data and then train the neural net with it:\n",
    "\n",
    "<code>X, y = load()\n",
    "net1.fit(X, y)</code>\n",
    "\n",
    "Running these two lines will output a table that grows one row per training epoch. In each row, we'll see the current loss (MSE) on the train set and on the validation set and the ratio between the two. NeuralNet automatically splits the data provided in X into a training and a validation set, using 20% of the samples for validation. (We can adjust this ratio by overriding the eval_size=0.2 parameter.)\n",
    "\n",
    "![4](img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the validation loss keeps improving until the end. (If we let it train longer, it will improve a little more.)\n",
    "\n",
    "### <font color='blue'>Testing it out</font>\n",
    "\n",
    "The net1 object actually keeps a record of the data that it prints out in the table. We can access that record through the train_history_ attribute. Let's draw those two curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net1.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net1.train_history_])\n",
    "pyplot.plot(train_loss, linewidth=3, label=\"train\")\n",
    "pyplot.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "pyplot.grid()\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"epoch\")\n",
    "pyplot.ylabel(\"loss\")\n",
    "pyplot.ylim(1e-3, 1e-2)\n",
    "pyplot.yscale(\"log\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our net overfits, but it's not that bad\n",
    "\n",
    "The 'early stopping' technique (commonly used to avoid overfitting) would be very useful at this point\n",
    "\n",
    "Here we didn't use any regularization whatsoever, apart from choosing a small number of neurons in the hidden layer, a setting that will keep overfitting somewhat in control.\n",
    "\n",
    "Let's pick a few examples from the test set and check how do the net's predictions look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(x, y, axis):\n",
    "    img = x.reshape(96, 96)\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10)\n",
    "\n",
    "X, _ = load(test=True)\n",
    "y_pred = net1.predict(X)\n",
    "\n",
    "fig = pyplot.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(\n",
    "    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(16):\n",
    "    ax = fig.add_subplot(4, 4, i + 1, xticks=[], yticks=[])\n",
    "    plot_sample(X[i], y_pred[i], ax)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6](img/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions look reasonable, but sometimes they are quite a bit off. Let's try and do a bit better.\n",
    "\n",
    "###  Second model: convolutions\n",
    "\n",
    "![7](img/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5-style convolutional neural nets are at the heart of deep learning's recent breakthrough in computer vision. Convolutional layers are different to fully connected layers; they use a few tricks to reduce the number of parameters that need to be learned, while retaining high expressiveness. These are:\n",
    "\n",
    "* local connectivity: neurons are connected only to a subset of neurons in the previous layer,\n",
    "*  weight sharing: weights are shared between a subset of neurons in the convolutional layer (these neurons form what's called a feature map),\n",
    "*  pooling: static subsampling of inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![8](img/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Units in a convolutional layer actually connect to a 2-d patch of neurons in the previous layer, a prior that lets them exploit the 2-d structure in the input.\n",
    "\n",
    "When using convolutional layers in Lasagne, we have to prepare the input data such that each sample is no longer a flat vector of 9,216 pixel intensities, but a three-dimensional matrix with shape (c, 0, 1), where c is the number of channels (colors), and 0 and 1 correspond to the x and y dimensions of the input image. In our case, the concrete shape will be (1, 96, 96), because we're dealing with a single (gray) color channel only.\n",
    "\n",
    "A function load2d that wraps the previously written load and does the necessary transformations is easily coded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load2d(test=False, cols=None):\n",
    "    X, y = load(test=test)\n",
    "    X = X.reshape(-1, 1, 96, 96)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build a convolutional neural net with three convolutional layers and two fully connected layers. Each conv layer is followed by a 2x2 max-pooling layer. Starting with 32 filters, we double the number of filters with every conv layer. The densely connected hidden layers both have 500 units.\n",
    "\n",
    "There's again no regularization in the form of weight decay or dropout. It turns out that using very small convolutional filters, such as our 3x3 and 2x2 filters, is again a pretty good regularizer by itself.\n",
    "\n",
    "Let's write down the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = NeuralNet(\n",
    "    layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "        ('conv1', layers.Conv2DLayer),\n",
    "        ('pool1', layers.MaxPool2DLayer),\n",
    "        ('conv2', layers.Conv2DLayer),\n",
    "        ('pool2', layers.MaxPool2DLayer),\n",
    "        ('conv3', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('hidden5', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    input_shape=(None, 1, 96, 96),\n",
    "    conv1_num_filters=32, conv1_filter_size=(3, 3), pool1_pool_size=(2, 2),\n",
    "    conv2_num_filters=64, conv2_filter_size=(2, 2), pool2_pool_size=(2, 2),\n",
    "    conv3_num_filters=128, conv3_filter_size=(2, 2), pool3_pool_size=(2, 2),\n",
    "    hidden4_num_units=500, hidden5_num_units=500,\n",
    "    output_num_units=30, output_nonlinearity=None,\n",
    "\n",
    "    update_learning_rate=0.01,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=True,\n",
    "    max_epochs=1000,\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "X, y = load2d()  # load 2-d data\n",
    "net2.fit(X, y)\n",
    "\n",
    "# Training for 1000 epochs will take a while.  We'll pickle the\n",
    "# trained model so that we can load it back later:\n",
    "import cPickle as pickle\n",
    "with open('net2.pickle', 'wb') as f:\n",
    "    pickle.dump(net2, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this neural net is much more computationally costly than the first one we trained. It takes around 15x as long to train; those 1000 epochs take more than 20 minutes on even a powerful GPU.\n",
    "\n",
    "However, our patience is rewarded with what's already a much better model than the one we had before. Let's take a look at the output when running the script. First comes the list of layers with their output shapes. Note that the first conv layer produces 32 output images of size (94, 94), that's one 94x94 output image per filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputLayer            (None, 1, 96, 96)       produces    9216 outputs\n",
    "Conv2DCCLayer         (None, 32, 94, 94)      produces  282752 outputs\n",
    "MaxPool2DCCLayer      (None, 32, 47, 47)      produces   70688 outputs\n",
    "Conv2DCCLayer         (None, 64, 46, 46)      produces  135424 outputs\n",
    "MaxPool2DCCLayer      (None, 64, 23, 23)      produces   33856 outputs\n",
    "Conv2DCCLayer         (None, 128, 22, 22)     produces   61952 outputs\n",
    "MaxPool2DCCLayer      (None, 128, 11, 11)     produces   15488 outputs\n",
    "DenseLayer            (None, 500)             produces     500 outputs\n",
    "DenseLayer            (None, 500)             produces     500 outputs\n",
    "DenseLayer            (None, 30)              produces      30 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the predictions of the two networks using one of the more problematic samples in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = load(test=True)[0][6:7]\n",
    "sample2 = load2d(test=True)[0][6:7]\n",
    "y_pred1 = net1.predict(sample1)[0]\n",
    "y_pred2 = net2.predict(sample2)[0]\n",
    "\n",
    "fig = pyplot.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "plot_sample(sample1[0], y_pred1, ax)\n",
    "ax = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "plot_sample(sample1[0], y_pred2, ax)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![9](img/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's compare the learning curves of the first and the second network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10](img/10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good, I like the smoothness of the new error curves. But we do notice that towards the end, the validation error of net2 flattens out much more quickly than the training error. I bet we could improve that by using more training examples. What if we flipped the input images horizontically; would we be able to improve training by doubling the amount of training data this way?\n",
    "\n",
    "### <font color='blue'>Data augmentation</font>\n",
    "\n",
    "An overfitting net can generally be made to perform better by using more training data. (And if your unregularized net does not overfit, you should probably make it larger.)\n",
    "\n",
    "Data augmentation lets us artificially increase the number of training examples by applying transformations, adding noise etc. That's obviously more economic than having to go out and collect more examples by hand. Augmentation is a very useful tool to have in your deep learning toolbox.\n",
    "\n",
    "We mentioned batch iterators already briefly. It is the batch iterator's job to take a matrix of samples, and split it up in batches, in our case of size 128. While it does the splitting, the batch iterator can also apply transformations to the data on the fly. So to produce those horizontal flips, we don't actually have to double the amount of training data in the input matrix. Rather, we will just perform the horizontal flips with 50% chance while we're iterating over the data. This is convenient, and for some problems it allows us to produce an infinite number of examples, without blowing up the memory usage. Also, transformations to the input images can be done while the GPU is busy processing a previous batch, so they come at virtually no cost.\n",
    "\n",
    "Flipping the images horizontically is just a matter of using slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load2d()\n",
    "X_flipped = X[:, :, :, ::-1]  # simple slice to flip all images\n",
    "\n",
    "# plot two images:\n",
    "fig = pyplot.figure(figsize=(6, 3))\n",
    "ax = fig.add_subplot(1, 2, 1, xticks=[], yticks=[])\n",
    "plot_sample(X[1], y[1], ax)\n",
    "ax = fig.add_subplot(1, 2, 2, xticks=[], yticks=[])\n",
    "plot_sample(X_flipped[1], y[1], ax)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![11](img/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the picture on the right, notice that the target value keypoints aren't aligned with the image anymore. Since we're flipping the images, we'll have to make sure we also flip the target values. To do this, not only do we have to flip the coordinates, we'll also have to swap target value positions; that's because the flipped left_eye_center_x no longer points to the left eye in our flipped image; now it corresponds to right_eye_center_x. Some points like nose_tip_y are not affected. We'll define a tuple flip_indices that holds the information about which columns in the target vector need to swap places when we flip the image horizontically. Remember the list of columns was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_eye_center_x            7034\n",
    "left_eye_center_y            7034\n",
    "right_eye_center_x           7032\n",
    "right_eye_center_y           7032\n",
    "left_eye_inner_corner_x      2266\n",
    "left_eye_inner_corner_y      2266\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since left_eye_center_x will need to swap places with right_eye_center_x, we write down the tuple (0, 2). Also left_eye_center_y needs to swap places: with right_eye_center_y. Thus we write down (1, 3), and so on. In the end, we have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_indices = [\n",
    "    (0, 2), (1, 3),\n",
    "    (4, 8), (5, 9), (6, 10), (7, 11),\n",
    "    (12, 16), (13, 17), (14, 18), (15, 19),\n",
    "    (22, 24), (23, 25),\n",
    "    ]\n",
    "\n",
    "# Let's see if we got it right:\n",
    "df = read_csv(os.path.expanduser(FTRAIN))\n",
    "for i, j in flip_indices:\n",
    "    print(\"# {} -> {}\".format(df.columns[i], df.columns[j]))\n",
    "\n",
    "# this prints out:\n",
    "# left_eye_center_x -> right_eye_center_x\n",
    "# left_eye_center_y -> right_eye_center_y\n",
    "# left_eye_inner_corner_x -> right_eye_inner_corner_x\n",
    "# left_eye_inner_corner_y -> right_eye_inner_corner_y\n",
    "# left_eye_outer_corner_x -> right_eye_outer_corner_x\n",
    "# left_eye_outer_corner_y -> right_eye_outer_corner_y\n",
    "# left_eyebrow_inner_end_x -> right_eyebrow_inner_end_x\n",
    "# left_eyebrow_inner_end_y -> right_eyebrow_inner_end_y\n",
    "# left_eyebrow_outer_end_x -> right_eyebrow_outer_end_x\n",
    "# left_eyebrow_outer_end_y -> right_eyebrow_outer_end_y\n",
    "# mouth_left_corner_x -> mouth_right_corner_x\n",
    "# mouth_left_corner_y -> mouth_right_corner_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our batch iterator implementation will derive from the default BatchIterator class and override the transform() method only. Let's see how it looks like when we put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nolearn.lasagne import BatchIterator\n",
    "\n",
    "class FlipBatchIterator(BatchIterator):\n",
    "    flip_indices = [\n",
    "        (0, 2), (1, 3),\n",
    "        (4, 8), (5, 9), (6, 10), (7, 11),\n",
    "        (12, 16), (13, 17), (14, 18), (15, 19),\n",
    "        (22, 24), (23, 25),\n",
    "        ]\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "        Xb, yb = super(FlipBatchIterator, self).transform(Xb, yb)\n",
    "\n",
    "        # Flip half of the images in this batch at random:\n",
    "        bs = Xb.shape[0]\n",
    "        indices = np.random.choice(bs, bs / 2, replace=False)\n",
    "        Xb[indices] = Xb[indices, :, :, ::-1]\n",
    "\n",
    "        if yb is not None:\n",
    "            # Horizontal flip of all x coordinates:\n",
    "            yb[indices, ::2] = yb[indices, ::2] * -1\n",
    "\n",
    "            # Swap places, e.g. left_eye_center_x -> right_eye_center_x\n",
    "            for a, b in self.flip_indices:\n",
    "                yb[indices, a], yb[indices, b] = (\n",
    "                    yb[indices, b], yb[indices, a])\n",
    "\n",
    "        return Xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this batch iterator for training, we'll pass it as the batch_iterator_train argument to NeuralNet. Let's define net3, a network that looks exactly the same as net2 except for these lines at the very end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = NeuralNet(\n",
    "    # ...\n",
    "    regression=True,\n",
    "    batch_iterator_train=FlipBatchIterator(batch_size=128),\n",
    "    max_epochs=3000,\n",
    "    verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're passing our FlipBatchIterator, but we've also tripled the number of epochs to train. While each one of our training epochs will still look at the same number of examples as before (after all, we haven't changed the size of X), it turns out that training nevertheless takes quite a bit longer when we use our transforming FlipBatchIterator. This is because what the network learns generalizes better this time, and it's arguably harder to learn things that generalize than to overfit.\n",
    "\n",
    "So this will take maybe take an hour to train. Let's make sure we pickle the model at the end of training, and then we're ready to go fetch some tea and biscuits. Or maybe do the laundry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3.fit(X, y)\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('net3.pickle', 'wb') as f:\n",
    "    pickle.dump(net3, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$ python kfkd.py\n",
    "...\n",
    " Epoch  |  Train loss  |  Valid loss  |  Train / Val\n",
    "--------|--------------|--------------|----------------\n",
    "...\n",
    "   500  |    0.002238  |    0.002303  |     0.971519\n",
    "...\n",
    "  1000  |    0.001365  |    0.001623  |     0.841110\n",
    "  1500  |    0.001067  |    0.001457  |     0.732018\n",
    "  2000  |    0.000895  |    0.001369  |     0.653721\n",
    "  2500  |    0.000761  |    0.001320  |     0.576831\n",
    "  3000  |    0.000678  |    0.001288  |     0.526410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the learning with that of net2, we notice that the error on the validation set after 3000 epochs is indeed about 5% smaller for the data augmented net. We can see how net2 stops learning anything useful after 2000 or so epochs, and gets pretty noisy, while net3 continues to improve its validation error throughout, though slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![12](img/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Still seems like a lot of work for only a small gain? We'll find out if it was worth it in the next secion.\n",
    "### <font color='blue'>Changing learning rate and momentum over time</font>\n",
    "\n",
    "What's annoying about our last model is that it took already an hour to train it, and it's not exactly inspiring to have to wait for your experiment's results for so long. In this section, we'll talk about a combination of two tricks to fix that and make the net train much faster again.\n",
    "\n",
    "An intuition behind starting with a higher learning rate and decreasing it during the course of training is this: As we start training, we're far away from the optimum, and we want to take big steps towards it and learn quickly. But the closer we get to the optimum, the lighter we want to step. It's like taking the train home, but when you enter your door you do it by foot, not by train.\n",
    "\n",
    "**On the importance of initialization and momentum in deep learning** is the title of a talk and a paper by Ilya Sutskever et al. It's there that we learn about another useful trick to boost deep learning: namely increasing the optimization method's momentum parameter during training.\n",
    "\n",
    "Remember that in our previous model, we initialized learning rate and momentum with a static 0.01 and 0.9 respectively. Let's change that such that the learning rate decreases linearly with the number of epochs, while we let the momentum increase.\n",
    "\n",
    "NeuralNet allows us to update parameters during training using the on_epoch_finished hook. We can pass a function to on_epoch_finished and it'll be called whenever an epoch is finished. However, before we can assign new values to update_learning_rate and update_momentum on the fly, we'll have to change these two parameters to become Theano shared variables. Thankfully, that's pretty easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "\n",
    "def float32(k):\n",
    "    return np.cast['float32'](k)\n",
    "\n",
    "net4 = NeuralNet(\n",
    "    # ...\n",
    "    update_learning_rate=theano.shared(float32(0.03)),\n",
    "    update_momentum=theano.shared(float32(0.9)),\n",
    "    # ...\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callback or list of callbacks that we pass will be called with two arguments: nn, which is the NeuralNet instance itself, and train_history, which is the same as nn.train_history_.\n",
    "\n",
    "Instead of working with callback functions that use hard-coded values, we'll use a parametrizable class with a __call__ method as our callback. Let's call this class AdjustVariable. The implementation is reasonably straight-forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustVariable(object):\n",
    "    def __init__(self, name, start=0.03, stop=0.001):\n",
    "        self.name = name\n",
    "        self.start, self.stop = start, stop\n",
    "        self.ls = None\n",
    "\n",
    "    def __call__(self, nn, train_history):\n",
    "        if self.ls is None:\n",
    "            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\n",
    "\n",
    "        epoch = train_history[-1]['epoch']\n",
    "        new_value = float32(self.ls[epoch - 1])\n",
    "        getattr(nn, self.name).set_value(new_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plug it all together now and then we're ready to start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net4 = NeuralNet(\n",
    "    # ...\n",
    "    update_learning_rate=theano.shared(float32(0.03)),\n",
    "    update_momentum=theano.shared(float32(0.9)),\n",
    "    # ...\n",
    "    regression=True,\n",
    "    # batch_iterator_train=FlipBatchIterator(batch_size=128),\n",
    "    on_epoch_finished=[\n",
    "        AdjustVariable('update_learning_rate', start=0.03, stop=0.0001),\n",
    "        AdjustVariable('update_momentum', start=0.9, stop=0.999),\n",
    "        ],\n",
    "    max_epochs=3000,\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "X, y = load2d()\n",
    "net4.fit(X, y)\n",
    "\n",
    "with open('net4.pickle', 'wb') as f:\n",
    "    pickle.dump(net4, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train two nets: net4 doesn't use our FlipBatchIterator, net5 does. Other than that, they're identical.\n",
    "\n",
    "This is the learning of net4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch  |  Train loss  |  Valid loss  |  Train / Val\n",
    "--------|--------------|--------------|----------------\n",
    "    50  |    0.004216  |    0.003996  |     1.055011\n",
    "   100  |    0.003533  |    0.003382  |     1.044791\n",
    "   250  |    0.001557  |    0.001781  |     0.874249\n",
    "   500  |    0.000915  |    0.001433  |     0.638702\n",
    "   750  |    0.000653  |    0.001355  |     0.481806\n",
    "  1000  |    0.000496  |    0.001387  |     0.357917\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  Cool, training is happening much faster now! The train error at epochs 500 and 1000 is half of what it used to be in net2, before our adjustments to learning rate and momentum. This time, generalization seems to stop improving after 750 or so epochs already; looks like there's no point in training much longer.\n",
    "\n",
    "What about net5 with the data augmentation switched on?\n",
    "\n",
    " Epoch  |  Train loss  |  Valid loss  |  Train / Val\n",
    "--------|--------------|--------------|----------------\n",
    "    50  |    0.004317  |    0.004081  |     1.057609\n",
    "   100  |    0.003756  |    0.003535  |     1.062619\n",
    "   250  |    0.001765  |    0.001845  |     0.956560\n",
    "   500  |    0.001135  |    0.001437  |     0.790225\n",
    "   750  |    0.000878  |    0.001313  |     0.668903\n",
    "  1000  |    0.000705  |    0.001260  |     0.559591\n",
    "  1500  |    0.000492  |    0.001199  |     0.410526\n",
    "  2000  |    0.000373  |    0.001184  |     0.315353\n",
    "\n",
    "And again we have much faster training than with net3, and better results. After 1000 epochs, we're better off than net3 was after 3000 epochs. What's more, the model trained with data augmentation is now about 10% better with regard to validation error than the one without."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![13](img/13.png)\n",
    "\n",
    "### <font color='blue'>Dropout\n",
    "\n",
    "Introduced in 2012 in the Improving neural networks by preventing co-adaptation of feature detectors paper, dropout is a popular regularization technique that works amazingly well. I won't go into the details of why it works so well, you can read about that elsewhere.\n",
    "\n",
    "Like with any other regularization technique, dropout only makes sense if we have a network that's overfitting, which is clearly the case for the net5 network that we trained in the previous section. It's important to remember to get your net to train nicely and overfit first, then regularize.\n",
    "\n",
    "To use dropout with Lasagne, we'll add DropoutLayer layers between the existing layers and assign dropout probabilities to each one of them. Here's the complete definition of our new net. I've added a # ! comment at the end of those lines that were added between this and net5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net6 = NeuralNet(\n",
    "    layers=[\n",
    "        ('input', layers.InputLayer),\n",
    "        ('conv1', layers.Conv2DLayer),\n",
    "        ('pool1', layers.MaxPool2DLayer),\n",
    "        ('dropout1', layers.DropoutLayer),  # !\n",
    "        ('conv2', layers.Conv2DLayer),\n",
    "        ('pool2', layers.MaxPool2DLayer),\n",
    "        ('dropout2', layers.DropoutLayer),  # !\n",
    "        ('conv3', layers.Conv2DLayer),\n",
    "        ('pool3', layers.MaxPool2DLayer),\n",
    "        ('dropout3', layers.DropoutLayer),  # !\n",
    "        ('hidden4', layers.DenseLayer),\n",
    "        ('dropout4', layers.DropoutLayer),  # !\n",
    "        ('hidden5', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    input_shape=(None, 1, 96, 96),\n",
    "    conv1_num_filters=32, conv1_filter_size=(3, 3), pool1_pool_size=(2, 2),\n",
    "    dropout1_p=0.1,  # !\n",
    "    conv2_num_filters=64, conv2_filter_size=(2, 2), pool2_pool_size=(2, 2),\n",
    "    dropout2_p=0.2,  # !\n",
    "    conv3_num_filters=128, conv3_filter_size=(2, 2), pool3_pool_size=(2, 2),\n",
    "    dropout3_p=0.3,  # !\n",
    "    hidden4_num_units=500,\n",
    "    dropout4_p=0.5,  # !\n",
    "    hidden5_num_units=500,\n",
    "    output_num_units=30, output_nonlinearity=None,\n",
    "\n",
    "    update_learning_rate=theano.shared(float32(0.03)),\n",
    "    update_momentum=theano.shared(float32(0.9)),\n",
    "\n",
    "    regression=True,\n",
    "    batch_iterator_train=FlipBatchIterator(batch_size=128),\n",
    "    on_epoch_finished=[\n",
    "        AdjustVariable('update_learning_rate', start=0.03, stop=0.0001),\n",
    "        AdjustVariable('update_momentum', start=0.9, stop=0.999),\n",
    "        ],\n",
    "    max_epochs=3000,\n",
    "    verbose=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is sufficiently large now to crash Python's pickle with a maximum recursion error. Therefore we have to increase Python's recursion limit before we save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "X, y = load2d()\n",
    "net6.fit(X, y)\n",
    "\n",
    "import cPickle as pickle\n",
    "with open('net6.pickle', 'wb') as f:\n",
    "    pickle.dump(net6, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the learning, we notice that it's become slower again, and that's expected with dropout, but eventually it will outperform net5:\n",
    "\n",
    " Epoch  |  Train loss  |  Valid loss  |  Train / Val\n",
    "--------|--------------|--------------|---------------\n",
    "    50  |    0.004619  |    0.005198  |     0.888566\n",
    "   100  |    0.004369  |    0.004182  |     1.044874\n",
    "   250  |    0.003821  |    0.003577  |     1.068229\n",
    "   500  |    0.002598  |    0.002236  |     1.161854\n",
    "  1000  |    0.001902  |    0.001607  |     1.183391\n",
    "  1500  |    0.001660  |    0.001383  |     1.200238\n",
    "  2000  |    0.001496  |    0.001262  |     1.185684\n",
    "  2500  |    0.001383  |    0.001181  |     1.171006\n",
    "  3000  |    0.001306  |    0.001121  |     1.164100\n",
    "  \n",
    "  Also overfitting doesn't seem to be nearly as bad. Though we'll have to be careful with those numbers: the ratio between training and validation has a slightly different meaning now since the train error is evaluated with dropout, whereas the validation error is evaluated without dropout. A more comparable value for the train error is this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print mean_squared_error(net6.predict(X), y)\n",
    "\n",
    "prints something like 0.0010073791"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In our previous model without dropout, the error on the train set was 0.000373. So not only does our dropout net perform slightly better, it overfits much less than what we had before. That's great news, because it means that we can expect even better performance when we make the net larger (and more expressive). And that's what we'll try next: we increase the number of units in the last two hidden layers from 500 to 1000. Update these lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net7 = NeuralNet(\n",
    "    # ...\n",
    "    hidden4_num_units=1000,  # !\n",
    "    dropout4_p=0.5,\n",
    "    hidden5_num_units=1000,  # !\n",
    "    # ...\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The improvement over the non-dropout layer is now becoming more substantial:\n",
    "\n",
    " Epoch  |  Train loss  |  Valid loss  |  Train / Val\n",
    "--------|--------------|--------------|---------------\n",
    "    50  |    0.004756  |    0.007043  |     0.675330\n",
    "   100  |    0.004440  |    0.005321  |     0.834432\n",
    "   250  |    0.003974  |    0.003928  |     1.011598\n",
    "   500  |    0.002574  |    0.002347  |     1.096366\n",
    "  1000  |    0.001861  |    0.001613  |     1.153796\n",
    "  1500  |    0.001558  |    0.001372  |     1.135849\n",
    "  2000  |    0.001409  |    0.001230  |     1.144821\n",
    "  2500  |    0.001295  |    0.001146  |     1.130188\n",
    "  3000  |    0.001195  |    0.001087  |     1.099271\n",
    "\n",
    "And we're still looking really good with the overfitting! My feeling is that if we increase the number of epochs to train, this model might become even better. Let's try it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net12 = NeuralNet(\n",
    "    # ...\n",
    "    max_epochs=10000,\n",
    "    # ...\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Epoch  |  Train loss  |  Valid loss  |  Train / Val\n",
    "--------|--------------|--------------|---------------\n",
    "    50  |    0.004756  |    0.007027  |     0.676810\n",
    "   100  |    0.004439  |    0.005321  |     0.834323\n",
    "   500  |    0.002576  |    0.002346  |     1.097795\n",
    "  1000  |    0.001863  |    0.001614  |     1.154038\n",
    "  2000  |    0.001406  |    0.001233  |     1.140188\n",
    "  3000  |    0.001184  |    0.001074  |     1.102168\n",
    "  4000  |    0.001068  |    0.000983  |     1.086193\n",
    "  5000  |    0.000981  |    0.000920  |     1.066288\n",
    "  6000  |    0.000904  |    0.000884  |     1.021837\n",
    "  7000  |    0.000851  |    0.000849  |     1.002314\n",
    "  8000  |    0.000810  |    0.000821  |     0.985769\n",
    "  9000  |    0.000769  |    0.000803  |     0.957842\n",
    " 10000  |    0.000760  |    0.000787  |     0.966583\n",
    "\n",
    "So there you're witnessing the magic that is dropout. :-)\n",
    "\n",
    "Let's compare the nets we trained so for and their respective train and validation errors:\n",
    "\n",
    " Name  |   Description    |  Epochs  |  Train loss  |  Valid loss\n",
    "-------|------------------|----------|--------------|--------------\n",
    " net1  |  single hidden   |     400  |    0.002244  |    0.003255\n",
    " net2  |  convolutions    |    1000  |    0.001079  |    0.001566\n",
    " net3  |  augmentation    |    3000  |    0.000678  |    0.001288\n",
    " net4  |  mom + lr adj    |    1000  |    0.000496  |    0.001387\n",
    " net5  |  net4 + augment  |    2000  |    0.000373  |    0.001184\n",
    " net6  |  net5 + dropout  |    3000  |    0.001306  |    0.001121\n",
    " net7  |  net6 + epochs   |   10000  |    0.000760  |    0.000787\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
